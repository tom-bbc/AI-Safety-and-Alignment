# AI Safety and Alignment

### Papers

* [AI Alignment: A Comprehensive Survey](https://arxiv.org/abs/2310.19852) (Oct 2023)
* Anthropic:
  * [Alignment faking in large language models](https://www.anthropic.com/research/alignment-faking) (Dec 2024)
  * [Simple probes can catch sleeper agents](https://www.anthropic.com/research/probes-catch-sleeper-agents) (Apr 2024)
  * [Mapping the Mind of a Large Language Model](https://www.anthropic.com/research/mapping-mind-language-model) (May 2024)
* [Evaluating Frontier Models for Dangerous Capabilities](https://arxiv.org/abs/2403.13793) (Apr 2024)

### Blogposts

* Anthropic:
  * [Towards Monosemanticity: Decomposing Language Models With Dictionary Learning](https://transformer-circuits.pub/2023/monosemantic-features/index.html) (Oct 2023)
  * [Circuit Tracing: Revealing Computational Graphs in Language Models](https://transformer-circuits.pub/2025/attribution-graphs/methods.html) (Mar 2025)
  * [On the Biology of a Large Language Model](https://transformer-circuits.pub/2025/attribution-graphs/biology.html) by Anthropic (Mar 2025)
* DeepMind Safety Research:
  * [AGI Safety and Alignment at Google DeepMind: A Summary of Recent Work](https://deepmindsafetyresearch.medium.com/agi-safety-and-alignment-at-google-deepmind-a-summary-of-recent-work-8e600aca582a) (Oct 2024)
  * [An Approach to Technical AGI Safety and Security](https://deepmindsafetyresearch.medium.com/an-approach-to-technical-agi-safety-and-security-25928819fbc6) (Apr 2025)
  * [Negative Results for Sparse Autoencoders On Downstream Tasks](https://deepmindsafetyresearch.medium.com/negative-results-for-sparse-autoencoders-on-downstream-tasks-and-deprioritising-sae-research-6cadcfc125b9) (Mar 2025)
* [Model Organisms of Misalignment: The Case for a New Pillar of Alignment Research](https://www.lesswrong.com/posts/ChDH335ckdvpxXaXX/model-organisms-of-misalignment-the-case-for-a-new-pillar-of-1) by Evan Hubinger (Aug 2023)
* [What Is The Alignment Problem?](https://www.lesswrong.com/posts/dHNKtQ3vTBxTfTPxu/what-is-the-alignment-problem) by John Wentworth (Jan 2025)
* [A multi-disciplinary view on AI safety research](https://www.alignmentforum.org/posts/opE6L8jBTTNAyaDbB/a-multi-disciplinary-view-on-ai-safety-research) by Roman Leventov (Feb 2023)

### Videos

* Princeton AI Alignment and Safety seminars (Mar-Dec 2024):
  * [Homepage](https://pli.princeton.edu/events/princeton-ai-alignment-and-safety-seminar)
  * [YouTube playlist](https://www.youtube.com/playlist?list=PLWRU-w8UhT6jNg64UfBB0VtlvI4Upe914)
  * [Reading list](https://docs.google.com/spreadsheets/d/1xaPjEsWBnlBI2maz6k64z11A99USU7ahaC2V615FGjQ/edit?gid=848424154#gid=848424154)
* [Stanford Center for AI Safety Annual Meeting 2024](https://www.cs.stanford.edu/events/affiliates-events/stanford-center-ai-safety-annual-meeting-2024) (Aug 2024) âœ…

### Courses

* [AI Safety Fundamentals: AI Alignment Fast-Track](https://course.aisafetyfundamentals.com/alignment-fast-track) by BlueDot Impact âœ…
* [AI Safety Fundamentals: AI Alignment](https://course.aisafetyfundamentals.com/alignment) by BlueDot Impact âœ…
* [Intro to ML Safety](https://course.mlsafety.org) ðŸŸ 
* [Introducing our short course on AGI safety](https://deepmindsafetyresearch.medium.com/introducing-our-short-course-on-agi-safety-1072adb7912c) by DeepMind Safety Research âœ…
* [AI Safety & Alignment (COS 597Q)](https://sites.google.com/view/cos598aisafety/) by Princeton University
* [AI Alignment (CSC2547)](https://alignment-w2024.notion.site/CSC2547-AI-Alignment-b44359978f3a4a8f95c90adb0a6e7d53) by University of Toronto
* [AI Safety Initiative Fellowship](https://docs.google.com/document/d/1BAw0oX4eyVBXvz_58MeAINmZqonIjHdrsXq9KX1_JFo) by Georgia Institute of Technology
* [The Turing Online Learning Platform](https://www.turing.ac.uk/courses?utm_source=LinkedIn&utm_medium=Text_link&utm_campaign=Turing-Online-Learning-Platform)
