# AI Safety and Alignment

### Papers

* [AI Alignment: A Comprehensive Survey](https://arxiv.org/abs/2310.19852) (Oct 2023)
* [Alignment faking in large language models](https://www.anthropic.com/research/alignment-faking) (Dec 2024)
* [Simple probes can catch sleeper agents](https://www.anthropic.com/research/probes-catch-sleeper-agents) (Apr 2024)
* [Towards Monosemanticity: Decomposing Language Models With Dictionary Learning](https://transformer-circuits.pub/2023/monosemantic-features/index.html) (Oct 2023)

### Blogposts

* [Model Organisms of Misalignment: The Case for a New Pillar of Alignment Research](https://www.lesswrong.com/posts/ChDH335ckdvpxXaXX/model-organisms-of-misalignment-the-case-for-a-new-pillar-of-1) by Evan Hubinger (Aug 2023)
* [Towards Monosemanticity: Decomposing Language Models With Dictionary Learning](https://transformer-circuits.pub/2023/monosemantic-features/index.html) by Trenton Bricken (Oct 2023)
* [What Is The Alignment Problem?](https://www.lesswrong.com/posts/dHNKtQ3vTBxTfTPxu/what-is-the-alignment-problem) by John Wentworth (Jan 2025)
* [AGI Safety and Alignment at Google DeepMind: A Summary of Recent Work](https://deepmindsafetyresearch.medium.com/agi-safety-and-alignment-at-google-deepmind-a-summary-of-recent-work-8e600aca582a) by DeepMind Safety Research (Oct 2024)
* [A multi-disciplinary view on AI safety research](https://www.alignmentforum.org/posts/opE6L8jBTTNAyaDbB/a-multi-disciplinary-view-on-ai-safety-research) by Roman Leventov (Feb 2023)

### Videos

* Princeton AI Alignment and Safety seminars (Mar-Dec 2024):
  * [Homepage](https://pli.princeton.edu/events/princeton-ai-alignment-and-safety-seminar)
  * [YouTube playlist](https://www.youtube.com/playlist?list=PLWRU-w8UhT6jNg64UfBB0VtlvI4Upe914)
  * [Reading list](https://docs.google.com/spreadsheets/d/1xaPjEsWBnlBI2maz6k64z11A99USU7ahaC2V615FGjQ/edit?gid=848424154#gid=848424154)
* [Stanford Center for AI Safety Annual Meeting 2024](https://www.cs.stanford.edu/events/affiliates-events/stanford-center-ai-safety-annual-meeting-2024) (Aug 2024)

### Courses

* [AI Alignment Fast-Track course](https://course.aisafetyfundamentals.com/alignment-fast-track) by BlueBot Impact
* [AI Alignment Course](https://course.aisafetyfundamentals.com/alignment) by BlueDot Impact
* [Intro to ML Safety](https://course.mlsafety.org)
* [Introducing our short course on AGI safety](https://deepmindsafetyresearch.medium.com/introducing-our-short-course-on-agi-safety-1072adb7912c) by DeepMind Safety Research
* [AI Safety &amp; Alignment (COS 597Q)](https://sites.google.com/view/cos598aisafety/) by Princeton University
* [AI Alignment (CSC2547)](https://alignment-w2024.notion.site/CSC2547-AI-Alignment-b44359978f3a4a8f95c90adb0a6e7d53) by University of Toronto
* [AI Safety Initiative Fellowship](https://docs.google.com/document/d/1BAw0oX4eyVBXvz_58MeAINmZqonIjHdrsXq9KX1_JFo) by Georgia Institute of Technology
* [The Turing Online Learning Platform](https://www.turing.ac.uk/courses?utm_source=LinkedIn&utm_medium=Text_link&utm_campaign=Turing-Online-Learning-Platform)
