# Princeton University AI Alignment and Safety Seminars

## Interpretability and Decomposition: Interpreting CLIP's Image Representation via Text-Based Decomposition

Sources:
* [Interpretability and Decomposition: Jacob Steinhardt, UC Berkeley | YouTube](https://www.youtube.com/watch?v=nT_Bhs8Al-Y&list=PLWRU-w8UhT6jNg64UfBB0VtlvI4Upe914)
* [Jacob Steinhardt, UC Berkeley | Princeton University](https://pli.princeton.edu/events/2024/jacob-steinhardt-uc-berkeley)

### Decomposition background

* **Component-wise decomposition:** different components in an input image can be represented as directions in activation space of the attention heads in a model that lead to the final representation.
* **Spatial decomposition:** different spatial parts of the image can also be represented by different directions.
* CLIP:
  * Embedding model that embeds images into a latent representation.
  * Embeds into a joint image-text space.
  * Vision transformer architecture containing multi-head self-attention layers and MLP layers.
* Key claims:
    1. CLIP's outputs are mainly generated by the model's attention heads (and its MLP layers are less relevant to interpretability).
    2. The model's attention heads decompose as a 3D sum.
    3. Different attention heads create task-specific subspaces of the embedding/latent space.

### Claim 1: Measuring the effects of each model layer

* We can use **ablation testing** to measure how significantly each layer affects the model's outputs.
* We ablate layers progressively and measure model accuracy.
* Layers can be replaced by their mean value s.t. they don't depend on the input.
* On CLIP, this method finds that only the last four attention layers significantly matter to model outputs.

### Claim 2: Decomposing attention contributions

* From breaking down CLIP's computation process mathematically, we can deduce that the model's outputs are approximately calculated by:

$$CLIP(I) \approx P \cdot Z_0 + \sum_{i,l,h}c_{i,l,h}$$

* Where:
  * $P$ and $Z_0$ are some scalar matrices (not important to interpretability).
  * $c_{i,l,h}$ is the contribution of:
    * Image patch $i$
    * Attention head $h$
    * Model layer $l$
* Here, components of the model's outputs decompose additatively, so can be linked back for attribution.
* Grouping by position for attribution:
  * We can inspect the contributions $c_i$ of a given patch of the input image $i$ to the output by summing over heads $h$ and layers $l$.
  * $c_i \approx \sum_{h,l}c_{i,h,l}$
  * If we take the dot product of the patch contribution and the full images caption (from CLIP's image-text pairs) we can observe how much this patch contributes to the generated caption.
  * Attribution scores can be turned into segmentations by thresholding.

### Claim 3: Decomposing across attention heads

* Similarly to above, we can group by attention head $(l, h)$ (i.e. head $h$ in layer $l$) to find the contribution of each head to the model's outputs.
* $c_{l,h} \approx \sum_{i}c_{i,l,h}$
* Each high-dimentional contribution vector $c_{l,h}$ can then be used to 'interpret' the contributions of the attention head $(l, h)$.
* We want to label each attention head with a text-interpretable 'basis'.
* Hence, given some activation we can decompose it into the activated basises (e.g. 0.7 * basis 1 + 0.2 * basis 2 etc.).
* TextSpan algorithm:
  * Dataset of images.
  * Library of text concepts, embedded into CLIP space.
  * For a given head $(l, h)$, compute the contribution vector $c_{l,h}$ for each image in the dataset.
  * Use PCA (principal component analysis) to find the direction in the embeddings space that exhibits the most variation in attention head contributions.
  * Identify the text concept this direction is associated with.
  * Find another direction that is orthogonal to the first with highest variance, and identify the associated text concept, and repeat (for the number of dimensions).
  * Continue until we have explained most of the total variation within the given contribution vectors in the embeddings space.
  * Repeating for each head $(l, h)$, this results in a list of directions and their associated text concepts for each attention head in the model.
  * These concepts are the 'specialised subspace' of each attention head.
* Using the algorithm on CLIP shows that attention heads specialise into interpretable subspaces.
